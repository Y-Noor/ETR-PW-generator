{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOF04b34cWesXVw/ulhHW9Y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Y-Noor/ETR-PW-generator/blob/master/attentionunet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RGeQcsex9BGW"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, backend as K"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def attention_gate(x, g, inter_shape):\n",
        "    # Input x: Feature map from the encoder\n",
        "    # Input g: Gating signal from the decoder\n",
        "    # inter_shape: Number of intermediate filters in the attention mechanism\n",
        "\n",
        "    x_val = k.int(x)\n",
        "    g_val = k.int(g)\n",
        "\n",
        "    phi_x = layers.Conv2D(inter_shape, kernel_size=1, strides=1, padding='same')(x_val)\n",
        "    phi_g = layers.Conv2D(inter_shape, kernel_size=1, strides=1, padding='same')(g_val)\n",
        "\n",
        "    add_xg = layers.add([theta_x, phi_g])\n",
        "\n",
        "    relu_xg = layers.Activation('relu')(add_xg)\n",
        "\n",
        "    phi_xg = layers.Conv2D(1, kernel_size=1, strides=1, padding='same')(relu_xg)\n",
        "\n",
        "    sigmoid_xg = layers.Activation('sigmoid')(phi_xg)\n",
        "\n",
        "    attention_coeffs = layers.multiply([x_val, sigmoid_xg])\n",
        "\n",
        "    return attention_coeffs\n"
      ],
      "metadata": {
        "id": "5G4Qw39kIyLa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From what I understand, there is a pattern of:\n",
        "\n",
        "> conv\n",
        "\n",
        "> conv -> send to decoder layer through attention gate\n",
        "\n",
        "> pool\n",
        "\n",
        "followed by:\n",
        "> upsample\n",
        "\n",
        "> upsample\n",
        "\n",
        "> concatenate with signal received from encoder\n"
      ],
      "metadata": {
        "id": "T0w67rEYMaL3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def upsampling(x):\n",
        "    return tf.image.resize(x, size=(n//2, n//2), method='nearest')\n",
        "\n",
        "# upsampled = layers.Lambda(upsampling)(input_tensor)\n"
      ],
      "metadata": {
        "id": "j7NW4uU_P-aX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def unet(input_shape, num_classes):\n",
        "    inputs = layers.Input(input_shape)\n",
        "\n",
        "    # encoder\n",
        "    conv1 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(inputs)\n",
        "    conv1 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(conv1)\n",
        "    pool1 = layers.MaxPooling2D(pool_size=(2, 2))(conv1)\n",
        "\n",
        "    conv2 = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(pool1)\n",
        "    conv2 = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(conv2)\n",
        "    pool2 = layers.MaxPooling2D(pool_size=(2, 2))(conv2)\n",
        "\n",
        "    conv3 = layers.Conv2D(256, (3, 3), activation='relu', padding='same')(pool2)\n",
        "    conv3 = layers.Conv2D(256, (3, 3), activation='relu', padding='same')(conv3)\n",
        "    pool3 = layers.MaxPooling2D(pool_size=(2, 2))(conv3)\n",
        "\n",
        "    conv4 = layers.Conv2D(512, (3, 3), activation='relu', padding='same')(pool3)\n",
        "    conv4 = layers.Conv2D(512, (3, 3), activation='relu', padding='same')(conv4)\n",
        "    pool4 = layers.MaxPooling2D(pool_size=(2, 2))(conv4)\n",
        "\n",
        "    # lowest depth\n",
        "    conv5 = layers.Conv2D(1024, (3, 3), activation='relu', padding='same')(pool4)\n",
        "    conv5 = layers.Conv2D(1024, (3, 3), activation='relu', padding='same')(conv5)\n",
        "\n",
        "\n",
        "\n",
        "    # decoder\n",
        "    up6 = layers.Lambda(upsampling)(conv5)\n",
        "    attn6 = attention_gate(conv4, up6, 512)\n",
        "    merge6 = layers.concatenate([up6, attn6], axis=3)\n",
        "    conv6 = layers.Conv2D(512, (3, 3), activation='relu', padding='same')(merge6)\n",
        "    conv6 = layers.Conv2D(512, (3, 3), activation='relu', padding='same')(conv6)\n",
        "\n",
        "    up7 = layers.Lambda(upsampling)(conv6)\n",
        "    attn7 = attention_gate(conv3, up7, 256)\n",
        "    merge7 = layers.concatenate([up7, attn7], axis=3)\n",
        "    conv7 = layers.Conv2D(256, (3, 3), activation='relu', padding='same')(merge7)\n",
        "    conv7 = layers.Conv2D(256, (3, 3), activation='relu', padding='same')(conv7)\n",
        "\n",
        "    up8 = layers.Lambda(upsampling)(conv7)\n",
        "    attn8 = attention_gate(conv2, up8, 128)\n",
        "    merge8 = layers.concatenate([up8, attn8], axis=3)\n",
        "    conv8 = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(conv8)\n",
        "    conv8 = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(conv8)\n",
        "\n",
        "    up9 = layers.Lambda(upsampling)(conv8)\n",
        "    attn9 = attention_gate(conv1, up9, 64)\n",
        "    merge9 = layers.concatenate([up9, attn9], axis=3)\n",
        "    conv9 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(merge9)\n",
        "    conv9 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(conv9)\n",
        "\n",
        "    # Output layer\n",
        "    conv10 = layers.Conv2D(num_classes, (1, 1), activation='softmax')(conv9)\n",
        "\n",
        "    model = models.Model(inputs=inputs, outputs=conv10)\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "9C5ruEZAL5P4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ClpqevANQeWc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}